{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776a820-a59d-4106-81c4-47901f7d0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Src\n",
    "# Read Kafka stream -- Target Postgres DB\n",
    "# Read Batch file -- Target Postgres DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818e9c7d-0e60-4d70-a504-e0d4041783ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary==2.9.9 in c:\\users\\russe\\anaconda3\\lib\\site-packages (2.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary==2.9.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e516aef-965e-469d-ae04-ce587919f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed table creation successfully\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    port=5433,\n",
    "    database=\"de_proj\",\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "def try_execute_sql(sql: str):\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        conn.commit()\n",
    "        print(f\"Executed table creation successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't execute table creation due to exception: {e}\")\n",
    "        conn.rollback()\n",
    "    # Close the cursor and connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "def create_table():\n",
    "    \"\"\"\n",
    "    Creates the sensorInfo table and its columns.\n",
    "    \"\"\"\n",
    "    # SQL to create a table\n",
    "    create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stg_SensorInfo (\n",
    "            record JSONB,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \n",
    "        CREATE TABLE IF NOT EXISTS stg_SensorInfo_json (\n",
    "            record JSONB,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \n",
    "        CREATE TABLE IF NOT EXISTS SensorInfo (\n",
    "            id INT GENERATED ALWAYS AS IDENTITY PRIMARY KEY,\n",
    "            timestamp TIMESTAMP NOT NULL,\n",
    "            sensor_id INTEGER,\n",
    "            value DOUBLE PRECISION,\n",
    "            city VARCHAR(50),\n",
    "            country VARCHAR(50),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "    try_execute_sql(create_table_sql)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf188195-1c18-43c8-b0f1-7447228ecce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def postgres_conn():\n",
    "    # Database connection settings\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"127.0.0.1\",\n",
    "        port=5433,\n",
    "        database=\"de_proj\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\"\n",
    "    )\n",
    "    \n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe2d3d6-35a3-4212-8701-ffefc307bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in c:\\users\\russe\\anaconda3\\lib\\site-packages (2.2.7)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceacad84-d2c4-42c0-b9c3-c9da1a781d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending: {'timestamp': 1747800450, 'sensor_id': 3, 'value': 38.10958339372211, 'city': 'Langley', 'country': 'BC'}\n",
      "Sending: {'timestamp': 1747800451, 'sensor_id': 5, 'value': 86.47944019318841, 'city': 'Whitby', 'country': 'ON'}\n",
      "Sending: {'timestamp': 1747800452, 'sensor_id': 5, 'value': 55.68616192337594, 'city': 'Whitby', 'country': 'ON'}\n",
      "Sending: {'timestamp': 1747800453, 'sensor_id': 3, 'value': 72.73500269331272, 'city': 'Whitby', 'country': 'ON'}\n",
      "Sending: {'timestamp': 1747800454, 'sensor_id': 5, 'value': 60.816383297303844, 'city': 'Whitby', 'country': 'ON'}\n",
      "Sending: {'timestamp': 1747800455, 'sensor_id': 5, 'value': 39.659161419315446, 'city': 'Whitby', 'country': 'ON'}\n",
      "Sending: {'timestamp': 1747800456, 'sensor_id': 5, 'value': 26.367020153334806, 'city': 'Langley', 'country': 'BC'}\n",
      "Sending: {'timestamp': 1747800457, 'sensor_id': 1, 'value': 19.305507618278938, 'city': 'Whitby', 'country': 'ON'}\n",
      "Sending: {'timestamp': 1747800458, 'sensor_id': 1, 'value': 75.43290730206103, 'city': 'Langley', 'country': 'BC'}\n",
      "Sending: {'timestamp': 1747800459, 'sensor_id': 5, 'value': 41.8993447126642, 'city': 'Whitby', 'country': 'ON'}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Kafka Producer Configuration\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',  # broker IP\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Simulate producing data\n",
    "def generate_data():\n",
    "    location_dict = {\n",
    "    'ON': 'Ajax',\n",
    "    'ON': 'Milton',\n",
    "    'ON': 'Oakville',\n",
    "    'ON': 'Whitby',\n",
    "    'BC': 'Port Moody',\n",
    "    'BC': 'Delta',\n",
    "    'BC': 'Burnaby',\n",
    "    'BC': 'Langley'\n",
    "    }\n",
    "\n",
    "    selected_key, selected_value = random.choice(list(location_dict.items()))\n",
    "    \n",
    "    data = {\n",
    "        'timestamp': int(time.time()),\n",
    "        'sensor_id': random.randint(1, 5),\n",
    "        'value': random.random() * 100,\n",
    "        'city': selected_value,\n",
    "        'country': selected_key\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# Produce data to input-topic\n",
    "def produce_data():\n",
    "    count=0\n",
    "    #while True:\n",
    "    while count<10:\n",
    "        data = generate_data()\n",
    "        print(f\"Sending: {data}\")\n",
    "        producer.send('sensorInfo', value=data)\n",
    "        time.sleep(1)\n",
    "        count +=1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    produce_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eca7315-af6c-42b1-9f84-16d300f74aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polling messages for 10 seconds...\n",
      "Done. Collected 10 messages.\n",
      "Data written to file:  {'data/kafka_messages.json'}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import time\n",
    "\n",
    "# Connect to Kafka and subscribe to topic\n",
    "consumer = KafkaConsumer(\n",
    "    'sensorInfo',                           # Replace with your topic\n",
    "    bootstrap_servers='localhost:9092',     # Match your Docker setup\n",
    "    auto_offset_reset='earliest',           # Start from the beginning if no offset\n",
    "    group_id='consumer-group',           # Consumer group ID\n",
    "    enable_auto_commit=True,                # Commit offsets automatically\n",
    "    value_deserializer=lambda x: x.decode('utf-8')  # Decode bytes to string\n",
    ")\n",
    "\n",
    "# Continuously listen for messages\n",
    "def de_kafka_consumer():\n",
    "    # List to store messages\n",
    "    messages = []\n",
    "    \n",
    "    # Poll for messages during a time window (e.g., 10 seconds)\n",
    "    timeout_secs = 10\n",
    "    end_time = time.time() + timeout_secs\n",
    "    \n",
    "    print(f\"Polling messages for {timeout_secs} seconds...\")\n",
    "    \n",
    "    while time.time() < end_time:\n",
    "        # Poll with short timeout to collect messages incrementally\n",
    "        polled = consumer.poll(timeout_ms=10)\n",
    "    \n",
    "        for topic_partition, records in polled.items():\n",
    "            for record in records:\n",
    "                #print(f\"Received: {record.value}\")\n",
    "                messages.append(record.value)\n",
    "\n",
    "    print(f\"Done. Collected {len(messages)} messages.\")\n",
    "\n",
    "    import json\n",
    "\n",
    "    filepath=\"data/kafka_messages.json\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(messages, f, indent=2)\n",
    "        \n",
    "    print(f\"Data written to file: \",{filepath})\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    de_kafka_consumer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b6386bf-feb6-4de5-a65e-fb70e40dd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3688586-94e3-4c64-91fa-4823c56feb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747787759 4 56.77640368566098 Whitby ON\n",
      "1747787761 2 59.12027390887851 Langley BC\n",
      "1747787762 2 62.66116270625716 Langley BC\n",
      "1747787763 5 2.343682598834418 Whitby ON\n",
      "1747787764 1 87.08696766999357 Whitby ON\n",
      "1747787765 4 20.357644775637162 Whitby ON\n",
      "1747787766 5 1.047685939513776 Langley BC\n",
      "1747787767 4 97.82147529631257 Langley BC\n",
      "1747787768 5 92.92027866265691 Whitby ON\n",
      "1747787769 4 71.83390551365967 Langley BC\n",
      "1747789394 3 49.278125440360796 Whitby ON\n",
      "1747789396 5 93.89550366396267 Langley BC\n",
      "1747789397 5 73.54078772029678 Whitby ON\n",
      "1747789398 1 97.1560479452384 Whitby ON\n",
      "1747789399 3 69.78298885556862 Whitby ON\n",
      "1747789400 5 35.483795180988544 Whitby ON\n",
      "1747789401 2 36.9241120094266 Whitby ON\n",
      "1747789402 2 77.28465396865352 Whitby ON\n",
      "1747789403 4 19.165706730030664 Whitby ON\n",
      "1747789404 5 56.65409248907063 Whitby ON\n",
      "Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def transform_data():\n",
    "    filepath=\"data/kafka_messages.json\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        content = f.read()\n",
    "        data = json.loads(content)\n",
    "   \n",
    "    #print(data)  # This will be a Python list\n",
    "    \n",
    "    conn = postgres_conn()\n",
    "    cur= conn.cursor()\n",
    "    \n",
    "    #Insert data into table\n",
    "    for item in data:\n",
    "        record = json.loads(item)  # parse each string in the list\n",
    "        print(record['timestamp'],record['sensor_id'], record['value'],record['city'], record['country'])\n",
    "        cur.execute(\n",
    "            \"INSERT INTO SensorInfo (timestamp,sensor_id,value,city,country) VALUES (TO_TIMESTAMP(%s), %s, %s, %s, %s) ON CONFLICT (id) DO NOTHING\",\n",
    "            (record['timestamp'],record['sensor_id'], record['value'],record['city'], record['country'])\n",
    "        )\n",
    "    \n",
    "    #Commit and close\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Data inserted successfully.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    transform_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dbe5d49-3571-4ac9-95db-7489aebc5087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Result:\n",
      "(1, datetime.datetime(2025, 5, 21, 0, 35, 59), 4, 56.77640368566098, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(2, datetime.datetime(2025, 5, 21, 0, 36, 1), 2, 59.12027390887851, 'Langley', 'BC', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(3, datetime.datetime(2025, 5, 21, 0, 36, 2), 2, 62.66116270625716, 'Langley', 'BC', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(4, datetime.datetime(2025, 5, 21, 0, 36, 3), 5, 2.343682598834418, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(5, datetime.datetime(2025, 5, 21, 0, 36, 4), 1, 87.08696766999357, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(6, datetime.datetime(2025, 5, 21, 0, 36, 5), 4, 20.357644775637162, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(7, datetime.datetime(2025, 5, 21, 0, 36, 6), 5, 1.047685939513776, 'Langley', 'BC', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(8, datetime.datetime(2025, 5, 21, 0, 36, 7), 4, 97.82147529631257, 'Langley', 'BC', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(9, datetime.datetime(2025, 5, 21, 0, 36, 8), 5, 92.92027866265691, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(10, datetime.datetime(2025, 5, 21, 0, 36, 9), 4, 71.83390551365967, 'Langley', 'BC', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(11, datetime.datetime(2025, 5, 21, 1, 3, 14), 3, 49.278125440360796, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(12, datetime.datetime(2025, 5, 21, 1, 3, 16), 5, 93.89550366396267, 'Langley', 'BC', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(13, datetime.datetime(2025, 5, 21, 1, 3, 17), 5, 73.54078772029678, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(14, datetime.datetime(2025, 5, 21, 1, 3, 18), 1, 97.1560479452384, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(15, datetime.datetime(2025, 5, 21, 1, 3, 19), 3, 69.78298885556862, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(16, datetime.datetime(2025, 5, 21, 1, 3, 20), 5, 35.483795180988544, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(17, datetime.datetime(2025, 5, 21, 1, 3, 21), 2, 36.9241120094266, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(18, datetime.datetime(2025, 5, 21, 1, 3, 22), 2, 77.28465396865352, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(19, datetime.datetime(2025, 5, 21, 1, 3, 23), 4, 19.165706730030664, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n",
      "(20, datetime.datetime(2025, 5, 21, 1, 3, 24), 5, 56.65409248907063, 'Whitby', 'ON', datetime.datetime(2025, 5, 21, 1, 4, 56, 324946))\n"
     ]
    }
   ],
   "source": [
    "#VALIDATION\n",
    "\n",
    "conn = postgres_conn()\n",
    "cur= conn.cursor()\n",
    "cur.execute(\n",
    "            \"SELECT * FROM SensorInfo;\"\n",
    "        )\n",
    "result = cur.fetchall()\n",
    "# Print the results to validate the insertion\n",
    "print(\"Validation Result:\")\n",
    "for row in result:\n",
    "    print(row)\n",
    "    \n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cab913c-abe9-4ea2-aeee-a6d4c59c4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e44f13-72a6-4946-b393-7187527df821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\russe\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\russe\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29fe758c-df50-4f57-a994-40bcf76bd359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5cc581c-d937-4202-b165-900bec619d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully\n",
      "C:\\Users\\russe\\Desktop\\JaniceProj\\GitProjects\\de_e2e\n",
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n",
      "write completed\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import os\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/hadoop/hadoop-3.3.5\"\n",
    "\n",
    "def create_spark_session() -> SparkSession:\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"Transform data with PySpark\")\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "        .master(\"local[*]\") \n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    print(\"Spark session created successfully\")\n",
    "    return spark\n",
    "\n",
    "    \n",
    "def spark_transform_data_load_table(filepath,spark_session):\n",
    "    import os\n",
    "    print(os.getcwd()) \n",
    "\n",
    "    # To allow automatic schemaInference while reading\n",
    "    spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "    \n",
    "    # Create the streaming_df to read from input directory\n",
    "    streaming_df = (\n",
    "        spark\n",
    "        .readStream\n",
    "        .format(\"json\")\n",
    "        .load(filepath)\n",
    "    )\n",
    "\n",
    "    # To the schema of the data, place a sample json file and change readStream to read \n",
    "    streaming_df.printSchema()\n",
    "\n",
    "    # Lets explode the data as devices contains list/array of device reading    \n",
    "    exploded_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "    \n",
    "    # Check the schema of the exploded_df, place a sample json file and change readStream to read \n",
    "    exploded_df.printSchema()\n",
    "\n",
    "    # Flatten the exploded df    \n",
    "    flattened_df = (\n",
    "        exploded_df\n",
    "        .drop(\"data\")\n",
    "        .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "        .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "        .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "        .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "        .drop(\"data_devices\")\n",
    "    )\n",
    "    # Check the schema of the flattened_df, place a sample json file and change readStream to read \n",
    "    flattened_df.printSchema()\n",
    "    \n",
    "    # Write the output to console sink to check the output\n",
    "    flattened_df.writeStream \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"path\", \"data/device_files/output/\") \\\n",
    "        .option(\"checkpointLocation\", \"output/stream_parquet/checkpoints/\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .start()\n",
    "    \n",
    "    print('write completed')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    filepath=\"data/device_files/input/device_data.json\"\n",
    "    spark=create_spark_session()\n",
    "    spark_transform_data_load_table(filepath,spark)\n",
    "    #transform_data_load_table(filepath)\n",
    "\n",
    "    # Stop the Spark session when done\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c3b45-dddd-4b3f-9e0f-5f59ca16cd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25533786-6c3d-418d-b5e5-6734c6364095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
